{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import logging\n",
    "importlib.reload(logging)\n",
    "import framework\n",
    "importlib.reload(framework)\n",
    "import infer_bert_classifier\n",
    "importlib.reload(infer_bert_classifier)\n",
    "import bert_utils\n",
    "importlib.reload(bert_utils)\n",
    "from framework import Usecase, DataCuration, FeatureEngineering, ModelEvaluator\n",
    "import pandas as pd\n",
    "import random\n",
    "# Define some constants and configurations\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ACCESS_TOKEN = 'WUpGevbWC9lsnTW8quNUtmWRdAEM89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the usecase details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "USECASE = 'ner'\n",
    "USECASE_CONFIG = {\n",
    "    'dataset': 'w2',\n",
    "    'num_labels': 3,\n",
    "    'labels_dict': {'person' : 0, 'org' : 1, 'none': 2}\n",
    "}\n",
    "\n",
    "usecase = Usecase(USECASE)\n",
    "usecase.set_config(USECASE_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths for datasets and goldens (local or ib, both work).\n",
    "Specify configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Loading dataset from /Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records\nINFO:root:Loading goldens from /Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv\nINFO:root:Total files Goldens: (154, 25)\nINFO:root:Total files found in the source: (142, 25)\n"
    }
   ],
   "source": [
    "DATAS = {\n",
    "    ('ner', 'w2') : '/Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records'\n",
    "}\n",
    "\n",
    "GOLDENS = {\n",
    "  ('ner', 'w2') : '/Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv'\n",
    "}\n",
    "\n",
    "DATASET = USECASE_CONFIG['dataset']\n",
    "DATA = DATAS[(USECASE, DATASET)]\n",
    "GOLDEN = GOLDENS[(USECASE, DATASET)]\n",
    "\n",
    "GOLDEN_CONFIG = {\n",
    "    'is_local': True,\n",
    "    'index_field_name':'filename',\n",
    "    'file_type': 'csv',\n",
    "    'skip_first_row': True,\n",
    "    'identifier': 'file'\n",
    "}\n",
    "DATASET_CONFIG = {\n",
    "    'is_local': True, \n",
    "    'file_type': 'ibocr',\n",
    "    'identifier': lambda path: os.path.basename(path).split('.ibocr')[0],\n",
    "    'convert2txt': False\n",
    "}\n",
    "\n",
    "w2 = DataCuration(ACCESS_TOKEN, DATA, DATASET_CONFIG, GOLDEN, GOLDEN_CONFIG)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Generating candidates for 142 files\nINFO:root:For X_DIST_THRESHOLD configuraion: 200\nINFO:root:total files: 142\nperson names found in candidates: 130\norg names found in candidates: 69\n\n"
    }
   ],
   "source": [
    "PROCESSING_CONFIG = {\n",
    "    'X_DIST_THRESHOLD': 200\n",
    "}\n",
    "CANDIDATES_FIELDS = {\n",
    "    'person':'employee_name',\n",
    "    'org':'employer_name'\n",
    "}\n",
    "w2.process_IBOCR_to_candidate_phrases(DATASET_CONFIG, PROCESSING_CONFIG)\n",
    "w2.compare_candidates_and_goldens(PROCESSING_CONFIG, CANDIDATES_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test data from goldens (from actual persons and company names) or from ibocr (using candidate phrases generated by processIBOCR2candidatePhrases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineering(usecase, w2, CANDIDATES_FIELDS)\n",
    "test_data_from_goldens = fe.generate_test_samples_from_goldens() # single dataframe\n",
    "test_data_from_candidates = fe.generate_test_samples_from_candidates() # dict{'filename' : dataframe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading fine-tuned model for inference. These models were separately trained using GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_TRAIN_DATA = 'w2'     # options for NER: (w2 or public)\n",
    "\n",
    "MODELS = {\n",
    "    ('ner', 'w2') : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/w2/no-address/5/model.pt',\n",
    "    ('ner', 'public') : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/public/no-address/200/model.pt'\n",
    "}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    'model_file_or_path' : MODELS[(USECASE, MODEL_TRAIN_DATA)],\n",
    "    'num_labels': USECASE_CONFIG['num_labels'],\n",
    "}\n",
    "\n",
    "EVAL_CONFIG = {\n",
    "    'gpu': False,\n",
    "    'use_goldens': False # True if test_data generated using generate_test_samples_from_goldens(), else False\n",
    "}\n",
    "test_data = test_data_from_candidates\n",
    "\n",
    "# ToDo\n",
    "# testing -- include as feature alongwith support with sampling from df\n",
    "test_data = dict(random.sample(test_data.items(), 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup model evaluator and evaluate either using test_data generated from goldens (test_data_from_goldens) or all candidate strings (test_data_from_candidates). Below code runs BERT inference and performs extraction, also calculating Recall, Precision, F1 by comparing with goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:inferring BERT classifier for file last_year_w2_1494968553744.PDF\nINFO:root:For field person, recall: 1.0000, precision: 0.2500, F1: 0.4000 \nINFO:root:For field org, recall: 1.0000, precision: 0.5000, F1: 0.6667 \nNumber of files:  1\n"
    }
   ],
   "source": [
    "\n",
    "model_evaluator = ModelEvaluator(usecase)\n",
    "model_evaluator.set_config(MODEL_CONFIG, EVAL_CONFIG)\n",
    "\n",
    "# Predictions (on test data)\n",
    "output = model_evaluator.run_evaluation(test_data)\n",
    "\n",
    "# Analyze results generated\n",
    "if EVAL_CONFIG['use_goldens']:\n",
    "    # output is a single df\n",
    "    print('Sample outputs: ', output.head())\n",
    "    model_evaluator.analyze_golden_result(output)\n",
    "else:\n",
    "    print('Number of files: ', len(output.keys()))\n",
    "    # output is a dictionary\n",
    "    results = model_evaluator.analyze_overall_result(output, w2.golden, CANDIDATES_FIELDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\n# Print results\\nfor typ in results:\\n    print('Field type: ', typ)\\n        for key in results[typ]:\\n            print('filename: ', key)\\n            print(results[typ][key])\\n\""
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "'''\n",
    "# Print results\n",
    "for typ in results:\n",
    "    print('Field type: ', typ)\n",
    "        for key in results[typ]:\n",
    "            print('filename: ', key)\n",
    "            print(results[typ][key])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ouputs from refiner flow (after step 4 producing single out.ibocr) and get extractions and metrics (Recall, Precision, F1) by comparing with goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REFINER_RESULTS = {\n",
    "    ('ner', 'w2') : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/refiner_results/w2.ibocr',\n",
    "    ('ner', 'resume') : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/refiner_results/resume.ibocr'\n",
    "}\n",
    "\n",
    "REFINER_RESULT_PATH = REFINER_RESULTS[(USECASE, MODEL_TRAIN_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:\nPerson Name Scores\nINFO:root:For model names_vontell, recall: 0.7465, precision: 0.4180, F1: 0.5359 \nINFO:root:For model names_token_matcher, recall: 0.6549, precision: 0.4602, F1: 0.5405 \nINFO:root:For model names_spacy, recall: 0.0915, precision: 0.0034, F1: 0.0066 \nINFO:root:\nOrg Name Scores\nINFO:root:For model org_spacy, recall: 0.0775, precision: 0.0012, F1: 0.0023 \n"
    }
   ],
   "source": [
    "model_evaluator = ModelEvaluator(usecase)\n",
    "results = model_evaluator.analyze_refiner_results(REFINER_RESULT_PATH, w2.golden, CANDIDATES_FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\" \\n# Print results\\nfor typ in results:\\n    print('Field type: ', typ)\\n    for model in results[typ]:\\n        print('model type: ', model)\\n        for key in results[typ][model]:\\n            print('filename: ', key)\\n            print(results[typ][model][key])\\n\""
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "''' \n",
    "# Print results\n",
    "for typ in results:\n",
    "    print('Field type: ', typ)\n",
    "    for model in results[typ]:\n",
    "        print('model type: ', model)\n",
    "        for key in results[typ][model]:\n",
    "            print('filename: ', key)\n",
    "            print(results[typ][model][key])\n",
    "'''            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595239351101",
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}