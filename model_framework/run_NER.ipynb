{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import logging\n",
    "importlib.reload(logging)\n",
    "import framework\n",
    "importlib.reload(framework)\n",
    "import infer_bert_classifier\n",
    "importlib.reload(infer_bert_classifier)\n",
    "import bert_utils\n",
    "importlib.reload(bert_utils)\n",
    "from framework import *\n",
    "import pandas as pd\n",
    "# Define some constants and configurations\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ACCESS_TOKEN = 'WUpGevbWC9lsnTW8quNUtmWRdAEM89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the task details. This notebook handles NER (for labeling person and company names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'w2' # supports w2 and resume\n",
    "TASK_CONFIG = {\n",
    "    'task': 'ner',\n",
    "    'num_labels': 3,\n",
    "    'labels_dict': {'person' : 0, 'org' : 1, 'none': 2}\n",
    "}\n",
    "\n",
    "task = Task_NER(TASK_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths for datasets and goldens (local or ib, both work).\n",
    "Specify configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Loading dataset from /Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records\nINFO:root:142 files loaded\nINFO:root:Loading goldens from /Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv\nINFO:root:Total files Goldens: (154, 25)\nINFO:root:Total files found in the source: (142, 25)\nINFO:root:Processing 142 IBOCR files to txt\n"
    }
   ],
   "source": [
    "GOLDEN_CONFIG = {\n",
    "    'path': '/Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv',\n",
    "    'is_local': True,\n",
    "    'index_field_name':'filename',\n",
    "    'file_type': 'csv',\n",
    "    'identifier': 'file'\n",
    "}\n",
    "DATASET_CONFIG = {\n",
    "    'path': '/Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records',\n",
    "    'is_local': True, \n",
    "    'file_type': 'ibocr',\n",
    "    'identifier': lambda path: os.path.basename(path).split('.ibocr')[0],\n",
    "    'convert2txt': True\n",
    "}\n",
    "\n",
    "data = DataCuration(ACCESS_TOKEN, DATASET_CONFIG, GOLDEN_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Generating candidates for 142 files\nINFO:root:For X_DIST_THRESHOLD configuraion: 200\nINFO:root:total files: 142\nperson names found in candidates: 130\norg names found in candidates: 69\n\n"
    }
   ],
   "source": [
    "PROCESSING_CONFIG = {\n",
    "    'X_DIST_THRESHOLD': 200\n",
    "}\n",
    "\n",
    "DATA_ARGS = {\n",
    "    'task': task,\n",
    "    'dataset': data,\n",
    "    'candidates_fields': {\n",
    "        'person':'employee_name',\n",
    "        'org':'employer_name'\n",
    "    }\n",
    "}\n",
    "\n",
    "data.generate_candidates_phrases(PROCESSING_CONFIG)\n",
    "data.compare_candidates_and_goldens(DATA_ARGS['candidates_fields'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate test data from goldens (from actual persons and company names) or from ibocr (using candidate phrases generated by processIBOCR2candidatePhrases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineering_NER(DATA_ARGS)\n",
    "test_data_from_goldens = fe.generate_test_samples_from_goldens() # single dataframe\n",
    "test_data_from_candidates = fe.generate_test_samples_from_candidates() # dict{'filename' : dataframe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading fine-tuned model for inference. These models were separately trained using GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DICT = {\n",
    "    'w2' : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/w2/no-address/5/model.pt', # trained on public w2 from Kaggle\n",
    "    'public': '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/public/no-address/200/model.pt' # trained on public names repo\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'task': task,\n",
    "    'model_file_or_path' : MODEL_DICT['w2'],\n",
    "    'num_labels': TASK_CONFIG['num_labels'],\n",
    "    'gpu': False,\n",
    "    'use_goldens': False # True if test_data generated using generate_test_samples_from_goldens(), else False\n",
    "}\n",
    "\n",
    "test_data = test_data_from_candidates # choose from test_data_from_goldens or test_data_from_candidates\n",
    "\n",
    "# Do only for debugging and getting quick results\n",
    "test_data = FeatureEngineering.get_subset_for_debugging(test_data, sample_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup model evaluator and evaluate either using test_data generated from goldens (test_data_from_goldens) or all candidate strings (test_data_from_candidates). Below code runs BERT inference and performs extraction, also calculating Recall, Precision, F1 by comparing with goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:inferring BERT classifier for file last_year_w2_1494968326840.PDF\nINFO:root:inferring BERT classifier for file last_year_w2_1493919686919.PDF\nINFO:root:For field person, recall: 1.0000, precision: 0.5000, F1: 0.6667 \nINFO:root:For field org, recall: 0.0000, precision: 0.5000, F1: 0.0000 \nNumber of files:  2\n"
    }
   ],
   "source": [
    "\n",
    "model_evaluator = ModelEvaluator(TRAINING_ARGS)\n",
    "\n",
    "# Predictions (on test data)\n",
    "output = model_evaluator.run_evaluation(test_data)\n",
    "\n",
    "# Analyze results generated\n",
    "if TRAINING_ARGS['use_goldens']:\n",
    "    # output is a single df\n",
    "    print('Sample outputs: ', output.head())\n",
    "    model_evaluator.analyze_golden_result(output)\n",
    "else:\n",
    "    print('Number of files: ', len(output.keys()))\n",
    "    # output is a dictionary\n",
    "    results = model_evaluator.analyze_overall_result(output, data.golden, DATA_ARGS['candidates_fields'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"\\n# Print results\\nfor typ in results:\\n    print('Field type: ', typ)\\n        for key in results[typ]:\\n            print('filename: ', key)\\n            print(results[typ][key])\\n\""
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "'''\n",
    "# Print results\n",
    "for typ in results:\n",
    "    print('Field type: ', typ)\n",
    "        for key in results[typ]:\n",
    "            print('filename: ', key)\n",
    "            print(results[typ][key])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ouputs from refiner flow (after step 4 producing single out.ibocr) and get extractions and metrics (Recall, Precision, F1) by comparing with goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "W2_REFINER_RESULT_PATH = '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/refiner_results/w2.ibocr'\n",
    "RESUME_REFINER_RESULT_PATH = '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/refiner_results/resume.ibocr'\n",
    "\n",
    "REFINER_RESULT_PATH = W2_REFINER_RESULT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:\nPerson Name Scores\nINFO:root:For model names_vontell, recall: 0.7465, precision: 0.4180, F1: 0.5359 \nINFO:root:For model names_token_matcher, recall: 0.6549, precision: 0.4602, F1: 0.5405 \nINFO:root:For model names_spacy, recall: 0.0915, precision: 0.0034, F1: 0.0066 \nINFO:root:\nOrg Name Scores\nINFO:root:For model org_spacy, recall: 0.0775, precision: 0.0012, F1: 0.0023 \n"
    }
   ],
   "source": [
    "model_evaluator = ModelEvaluator(TRAINING_ARGS)\n",
    "results = model_evaluator.analyze_refiner_results(REFINER_RESULT_PATH, data.golden, DATA_ARGS['candidates_fields'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\" \\n# Print results\\nfor typ in results:\\n    print('Field type: ', typ)\\n    for model in results[typ]:\\n        print('model type: ', model)\\n        for key in results[typ][model]:\\n            print('filename: ', key)\\n            print(results[typ][model][key])\\n\""
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "''' \n",
    "# Print results\n",
    "for typ in results:\n",
    "    print('Field type: ', typ)\n",
    "    for model in results[typ]:\n",
    "        print('model type: ', model)\n",
    "        for key in results[typ][model]:\n",
    "            print('filename: ', key)\n",
    "            print(results[typ][model][key])\n",
    "'''            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595482902340",
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}