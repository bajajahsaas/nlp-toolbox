{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models for IBDOCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import importlib\n",
    "import ibnlp\n",
    "importlib.reload(ibnlp)\n",
    "from ibnlp import PrototypeDataset, BERTUtils, IBDOCFeaturizer, ModelContext, product_dict, PUNC_TABLE\n",
    "\n",
    "# Define some constants and configurations\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ACCESS_TOKEN = 'INSERT IB TOKEN HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset + Goldens\n",
    "*Note that this can take a few minutes to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OMF_DATA = [\n",
    "    'ib_sales/OMF/fs/Instabase%20Drive/test/other-paystubs/Batch1/out/s3_map_records/',\n",
    "    'ib_sales/OMF/fs/Instabase%20Drive/test/other-paystubs/Batch2/out/s3_map_records/',\n",
    "    'ib_sales/OMF/fs/Instabase%20Drive/test/other-paystubs/Batch3/out/s1_process_files/',\n",
    "    'ib_sales/OMF/fs/Instabase%20Drive/test/other-paystubs/Batch4/out/s1_process_files/'\n",
    "]\n",
    "OMF_GOLDEN = [\n",
    "    './omf_batch_1.csv',\n",
    "    './omf_batch_2.csv',\n",
    "    './omf_batch_3.csv',\n",
    "    './omf_batch_4.csv'\n",
    "]\n",
    "OMF_MAPPING = ['ssn', 'last_4_ssn', 'per_end', 'pay_date', 'file', 'doc_type', 'template_name', 'per_begin', 'employee_name', 'employer_name', 'per_ss_tax', 'ytd_ss_tax', 'per_medicare_tax', 'ytd_medicare_tax', 'per_gross_pay', 'ytd_gross_pay', 'per_net_pay', 'ytd_net_pay']\n",
    "OMF_GOLDEN_CONFIG = {\n",
    "    'file_type': 'csv',\n",
    "    'skip_first_row': True,\n",
    "    'mapping': OMF_MAPPING,\n",
    "    'identifier': 'file'\n",
    "}\n",
    "OMF_DATASET_CONFIG = {\n",
    "    'file_type': 'ibdoc',\n",
    "    'identifier': lambda path: os.path.basename(path).split('.ibdoc')[0]\n",
    "}\n",
    "\n",
    "omf_paystubs = PrototypeDataset(ACCESS_TOKEN, OMF_DATA, OMF_DATASET_CONFIG, OMF_GOLDEN, OMF_GOLDEN_CONFIG)\n",
    "omf_paystubs.golden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model and Model Selection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Start by varying \n",
    "params = {\n",
    "    'batch_size': [32],#, 64, 128],\n",
    "    'epochs': [25, 50],\n",
    "    'max_num_tokens': [5],\n",
    "    'max_token_distance': [None],\n",
    "    #'embedding_type': ['glove', 'bert'],\n",
    "    'cardinal_only': [False],\n",
    "    'balance_targets': [True],\n",
    "    'additional_features': [[], ['is_number', 'is_company_indicator']],\n",
    "    'pre_processing': [[], ['lower_case', 'remove_punc'], ['lower_case'], ['remove_punc']]\n",
    "}\n",
    "\n",
    "def create_sample_data(dataset, model_context):\n",
    "    # Balance samples by removing some non-entity labeled datapoints\n",
    "    samples, targets, warnings = dataset.generate_spatial_samples('employer_name', context)\n",
    "    pos_idx = np.where(targets == 1)[0]\n",
    "    num_pos_samples = len(pos_idx)\n",
    "\n",
    "    neg_idxs_all = np.where(targets == 0)[0]\n",
    "    np.random.shuffle(neg_idxs_all)\n",
    "    neg_idx = neg_idxs_all[:num_pos_samples]\n",
    "\n",
    "    idx_to_use = np.concatenate((pos_idx, neg_idx))\n",
    "\n",
    "    filtered_samples = samples[idx_to_use]\n",
    "    filtered_targets = targets[idx_to_use]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(filtered_samples, filtered_targets, test_size=0.3, random_state=0)\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(X_train, X_test, y_train, y_test, model_context):\n",
    "\n",
    "    # Neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=model_context.epochs, batch_size=model_context.batch_size)\n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, history, model_context):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss']) \n",
    "    plt.title('Model loss') \n",
    "    plt.ylabel('Loss') \n",
    "    plt.xlabel('Epoch') \n",
    "    plt.legend(['Train', 'Test'], loc='upper left') \n",
    "    plt.show()\n",
    "    return history.history['val_acc'][-1]\n",
    "    \n",
    "    \n",
    "print(\"Trying on {} combinations of parameters\".format(len(list(product_dict(**params)))))\n",
    "results = []\n",
    "for parameters in product_dict(**params):\n",
    "    start = time.time()\n",
    "    context = ModelContext(**parameters)\n",
    "    print('Trying on following context:\\n\\t{}'.format(parameters))\n",
    "    samples = create_sample_data(omf_paystubs, context)\n",
    "    print('Input shape: {}'.format(samples[0].shape))\n",
    "    model, history = train_model(*samples, context)\n",
    "    acc = evaluate_model(model, history, context)\n",
    "    res = (acc, context, history, model, time.time() - start)\n",
    "    print((res[0], res[1], res[-1]))\n",
    "    results.append(res)\n",
    "    \n",
    "for i, result in enumerate(results):\n",
    "    print((i, result[0], result[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[np.argmax([r[0] for r in results])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some testing\n",
    "def evaluate(dataset, model, context, threshold=0.60, distance_threshold=1.5):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dataset_file in list(dataset.dataset.keys()):\n",
    "        try:\n",
    "            ibdoc = dataset.dataset[dataset_file].get_joined_page()[0] # 20, 54, 70\n",
    "            featurizer = IBDOCFeaturizer(ibdoc)\n",
    "            fvs = featurizer.get_feature_vectors(context)\n",
    "    #         print(ibdoc.get_text())\n",
    "    #         print('=================================')\n",
    "            predictions = model.predict(fvs)\n",
    "            predictions = predictions.tolist()\n",
    "            sequences = [[]]\n",
    "            for i, classification in enumerate(predictions):\n",
    "                if classification[0] > threshold:\n",
    "                    token_to_add = featurizer.get_all_tokens()[i]\n",
    "                    to_add_start, to_add_height = token_to_add['start_x'], token_to_add['line_height']\n",
    "                    if len(sequences[-1]) > 0 and (to_add_start - sequences[-1][-1]['end_x']) <= distance_threshold * to_add_height:\n",
    "                        sequences[-1].append(token_to_add)\n",
    "                    else:\n",
    "                        sequences.append([token_to_add])\n",
    "                elif len(sequences[-1]) > 0:\n",
    "                    sequences.append([])\n",
    "            companies = [' '.join([ss['word'] for ss in s]) for s in sequences if len(s) > 1]\n",
    "            results[dataset_file] = companies\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return results\n",
    "\n",
    "best_model_idx = np.argmax([r[0] for r in results])\n",
    "best_model = results[best_model_idx][3]\n",
    "best_context = results[best_model_idx][1]\n",
    "found_companies = evaluate(omf_paystubs, best_model, best_context)\n",
    "print(found_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "found_count = 0\n",
    "for cfile in found_companies:\n",
    "    try:\n",
    "        expected = omf_paystubs.golden.at[cfile, 'employer_name']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "    found = '\\n\\t\\t'.join(found_companies[cfile])\n",
    "    print(cfile[-10:])\n",
    "    print('\\t Found:\\n\\t\\t{}'.format(found))\n",
    "    print('\\t Expected:\\n\\t\\t{}'.format(expected))\n",
    "    if expected:  \n",
    "        total += 1\n",
    "    expected_san = expected.lower().strip().translate(PUNC_TABLE)\n",
    "    actual_san = [c.lower().strip().translate(PUNC_TABLE) for c in found_companies[cfile]]\n",
    "    is_contained = any([(expected_san in a) for a in actual_san]) or any([(a in expected_san) for a in actual_san])\n",
    "    if expected_san in actual_san or is_contained:\n",
    "        found_count += 1\n",
    "    else:\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "print(total)\n",
    "print(found_count)\n",
    "print('Recall: {}'.format(float(found_count)/float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omf_paystubs.evaluate({k: {'employer_name': found_companies[k]} for k in found_companies}, {}, fields=['employer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibdoc = omf_paystubs.dataset[list(omf_paystubs.dataset.keys())[54]].get_joined_page()[0] # 20, 54, 70\n",
    "featurizer = IBDOCFeaturizer(ibdoc)\n",
    "fvs = featurizer.get_feature_vectors(context)\n",
    "print(ibdoc.get_text())\n",
    "print('=================================')\n",
    "predictions = model.predict(fvs)\n",
    "predictions = predictions.tolist()\n",
    "sequences = [[]]\n",
    "for i, classification in enumerate(predictions):\n",
    "    if classification[0] > 0.99:\n",
    "        token_to_add = featurizer.get_all_tokens()[i]\n",
    "        to_add_start, to_add_height = token_to_add['start_x'], token_to_add['line_height']\n",
    "        if len(sequences[-1]) > 0 and (to_add_start - sequences[-1][-1]['end_x']) <= 1.5 * to_add_height:\n",
    "            sequences[-1].append(token_to_add)\n",
    "        else:\n",
    "            sequences.append([token_to_add])\n",
    "    elif len(sequences[-1]) > 0:\n",
    "        sequences.append([])\n",
    "companies = [' '.join([ss['word'] for ss in s]) for s in sequences if len(s) > 1]\n",
    "print(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
