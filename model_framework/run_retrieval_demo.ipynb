{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval\n",
    "- This notebook extracts 'search results' for queries from long natural language documents (CARTA Contracts)\n",
    "\n",
    "- Each huge document is split into multiple segments, each such segment can be a candidate for retrieval. \n",
    "\n",
    "- Works like any web search engine -- to return context surrounding the query terms. The algorithm matches each segment with query, and returns a ranked list of results (segments)\n",
    "\n",
    "- Uses text matching algorithm like BM25 -- https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "\n",
    "\n",
    "Final Intern presentation: https://docs.google.com/presentation/d/10mXA7K5sa_nAkqx2onsIfrH3TPj2Ni4LfCOxDhN5XBI/edit?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import logging\n",
    "import time\n",
    "importlib.reload(logging)\n",
    "import framework\n",
    "importlib.reload(framework)\n",
    "import bert_qa\n",
    "importlib.reload(bert_qa)\n",
    "import infer_bert_qa\n",
    "importlib.reload(infer_bert_qa)\n",
    "import bert_utils\n",
    "importlib.reload(bert_utils)\n",
    "import pandas as pd\n",
    "from framework import DataCuration, FeatureEngineering, StringProcessing\n",
    "from retrieval import TaskRetrieval, FeatureEngineeringRetrieval, Retrieval\n",
    "\n",
    "# Define some constants and configurations\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "ACCESS_TOKEN = 'WUpGevbWC9lsnTW8quNUtmWRdAEM89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Task\n",
    "- Mention configurations of the task and create a task object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'carta' # supports w2 and resume\n",
    "TASK_CONFIG = {\n",
    "    'task': 'retrieval'\n",
    "}\n",
    "\n",
    "task = TaskRetrieval(TASK_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate dataset\n",
    "- Specify paths for dataset. Paths can be local or from instabase drives (use *is_local*). \n",
    "- Limit data path to have just one document (for demo purpose of this notebook)\n",
    "- Also specify configurations like extensions, column names to use as index. \n",
    "- Currently supports csv format for goldens, ibocr/ibdoc for dataset. \n",
    "- Use *context2txt* to extract and store raw texts. \n",
    "- This object can be queried using *data.golden* or *data.dataset* or *data.dataset.texts* based on requirement\n",
    "\n",
    "### Files:\n",
    "- Download documents (ibocr files) from https://www.instabase.com/ib_solutions/solutions/fs/Instabase%20Drive/poc/carta/Annotated%20Sample/out/s1_process_files/ and specify local directory path. Use *is_local* as True\n",
    "- Alternatively, specify instabase drive path (/ib_solutions/solutions/fs/Instabase%20Drive/poc/carta/Annotated%20Sample/out/s1_process_files/) and set *is_local* as False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Loading dataset from /ib_solutions/solutions/fs/Instabase%20Drive/poc/carta/Annotated%20Sample/out/s1_process_files/\nINFO:root:5 files loaded\nINFO:root:Converting IBOCR/IBDOC to raw texts\n"
    }
   ],
   "source": [
    "CARTA_DATA = [\n",
    "#    '/Users/ahsaasbajaj/Documents/Data/QA_model/data'\n",
    "'/ib_solutions/solutions/fs/Instabase%20Drive/poc/carta/Annotated%20Sample/out/s1_process_files/'\n",
    "]\n",
    "\n",
    "DATASET_CONFIG = {\n",
    "    'path': CARTA_DATA,\n",
    "    'is_local': False, \n",
    "    'file_type': 'ibocr',\n",
    "    'identifier': lambda path: os.path.basename(path).split('.ibocr')[0],\n",
    "    'convert2txt': True\n",
    "}\n",
    "\n",
    "CARTA_GOLDEN = None\n",
    "GOLDEN_CONFIG = None\n",
    "\n",
    "data = DataCuration(ACCESS_TOKEN, DATASET_CONFIG, GOLDEN_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'annotated_AOI_2.pdf': <instabase.ocr.client.libs.ibocr.ParsedIBOCR at 0x15a56fa90>,\n 'annotated_AOI_3.pdf': <instabase.ocr.client.libs.ibocr.ParsedIBOCR at 0x15cb4fef0>,\n 'annotated_AOI_4.pdf': <instabase.ocr.client.libs.ibocr.ParsedIBOCR at 0x15a56f400>,\n 'annotated_AOI_5.pdf': <instabase.ocr.client.libs.ibocr.ParsedIBOCR at 0x15d39f080>}"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "data.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Generate Labeled Data)\n",
    "- Specify DATA_ARGS which includes the task and data objects created beforehand\n",
    "- Mention fields of interest (for extraction, classification) in DATA_ARGS\n",
    "- Split huge input document into multiple segments (each being candidate of retrieval)\n",
    "- Also generate maps which help to track page numbers and original doc indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Total pages in file: 20\nfilename to split and query:  annotated_AOI_4.pdf\nINFO:root:Total Corpus Size 247 docs\n"
    }
   ],
   "source": [
    "filename = 'annotated_AOI_4.pdf'\n",
    "print('filename to split and query: ', filename)\n",
    "\n",
    "query = \"Preferred Stocks\"\n",
    "\n",
    "NUM_FILES = len(data.dataset.keys())\n",
    "stime = time.time()\n",
    "\n",
    "DATA_ARGS = {\n",
    "    'task': task,\n",
    "    'dataset': data\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "'model_file_or_path': \"BM25Okapi\"\n",
    "}\n",
    "\n",
    "fe = FeatureEngineeringRetrieval(DATA_ARGS)\n",
    "\n",
    "corpus, doc_to_id_map, doc_to_pageNumber_map = fe.split_doc(filename=filename, split_size=100)  # list of document segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (BM25 Retrieval)\n",
    "- Specify model type \n",
    "- Specify queries to be inferred\n",
    "- This model uses BM25 text matching to rank results wrt query\n",
    "\n",
    "### Specify TRAINING_ARGS\n",
    "- Mention the class of model, to be used appropriately by back-end libraries\n",
    "\n",
    "\n",
    "#### Specify query (can be a single string or a list of queries) for question answering\n",
    "#### This block of code gets answer for *query* for document in *CARTA_DATA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Scores available for 247 docs\nINFO:root:Total time 1.4929239749908447 seconds\n"
    }
   ],
   "source": [
    "tokenized_corpus = fe.tokenize_corpus(corpus)\n",
    "\n",
    "model = Retrieval(DATA_ARGS, TRAINING_ARGS)\n",
    "model.train(corpus, tokenized_corpus, doc_to_id_map, doc_to_pageNumber_map)\n",
    "output, scores, top_pageNumbers = model.predict(query, len_results=5)\n",
    "\n",
    "etime = time.time()\n",
    "logging.info('Total time {} seconds'.format(etime - stime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Ranked Item 1 Matching score: 1.64, Page Number: 4\nPrices Certificate\" the \"Original Issue Price\" shall mean $0.795 per share for holders of Series Seed Preferred Stock, $1.3049 per share forholders of Series A Preferred Stock and $2.3199 for holders of Series A-1 Preferred Stock (each asadjusted for any stock dividends, stocksplits, stock combinations, recapitalizations orsimilar events withrespect tosuch shares) \n\nRanked Item 2 Matching score: 1.64, Page Number: 8\nThe \"Conversion Price\" for each series of Preferred Stock shall initially mean the Original Issue Price for such series of Preferred Stock. \n\nRanked Item 3 Matching score: 1.61, Page Number: 6\nIf any such holder shall be deemed to have converted such shares of Preferred Stock into Common Stock pursuant to this paragraph, then such holder shall not be entitled to receive any distribution that would otherwise be made to holders of the Series Seed Preferred Stock, Series A Preferred Stock or Series A-1 Preferred Stock (as applicable) that have not converted (or have not been deemed to have converted) into shares of Common Stock. \n\nRanked Item 4 Matching score: 1.53, Page Number: 8\nEach share of Preferred Stock \n\nRanked Item 5 Matching score: 1.53, Page Number: 3\nThe Preferred Stock shall be divided into series. \n\n"
    }
   ],
   "source": [
    "idx = 1\n",
    "for doc, score, page_num in zip(output, scores, top_pageNumbers):\n",
    "    print('Ranked Item {0} Matching score: {1:0.2f}, Page Number: {2}'.format(idx, score, page_num))\n",
    "    clean_output = \" \".join(doc.split())\n",
    "    print(clean_output, '\\n')\n",
    "    idx += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598385435274",
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}