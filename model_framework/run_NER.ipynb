{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER - extracting person and company names </br>\n",
    "- This notebook infers a trained BERT model and finds out person and company names from documents (w2, resumes, etc). \n",
    "\n",
    "- The model was separately trained using train_NER.ipynb or similar scripts (which uses huggingface library)\n",
    "\n",
    "- Modeled as multi-label classification problem (3 classes being - person, org, none)\n",
    "\n",
    "### Documents:\n",
    "\n",
    "- Complete Results: https://docs.google.com/spreadsheets/d/1rzEQrqRDqQpZ95_G95Fl1MKQppWJ1lAPBuOU6LVNiXY/edit?usp=sharing\n",
    "\n",
    "- Slides: https://docs.google.com/presentation/d/1EdubLHYdHDPJKT1GYHjkP86ZJMqjEhM_PFhvmWXCHSg/edit?usp=sharing\n",
    "\n",
    "- Final Intern presentation: https://docs.google.com/presentation/d/10mXA7K5sa_nAkqx2onsIfrH3TPj2Ni4LfCOxDhN5XBI/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/ahsaasbajaj/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import logging\n",
    "importlib.reload(logging)\n",
    "import framework\n",
    "importlib.reload(framework)\n",
    "import bert_ner\n",
    "importlib.reload(bert_ner)\n",
    "import infer_bert_classifier\n",
    "importlib.reload(infer_bert_classifier)\n",
    "import bert_utils\n",
    "importlib.reload(bert_utils)\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "from framework import DataCuration, FeatureEngineering\n",
    "from bert_ner import TaskNER, FeatureEngineeringNER, BERTNER\n",
    "\n",
    "# Define some constants and configurations\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "ACCESS_TOKEN = 'WUpGevbWC9lsnTW8quNUtmWRdAEM89'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Task\n",
    "- Mention configurations of the task and create a task object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'w2' # supports w2 and resume\n",
    "TASK_CONFIG = {\n",
    "    'task': 'ner',\n",
    "    'num_labels': 3,\n",
    "    'labels_dict': {'person' : 0, 'org' : 1, 'none': 2}\n",
    "}\n",
    "\n",
    "task = TaskNER(TASK_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate dataset\n",
    "- Specify paths for dataset and goldens (if available). Paths can be local or from instabase drives (use *is_local*). \n",
    "- Also specify configurations like extensions, column names to use as index. \n",
    "- Currently supports csv format for goldens, ibocr/ibdoc for dataset. \n",
    "- Use *context2txt* to extract and store raw texts. \n",
    "- This block creates a object of DataCuration() which maps goldens with the dataset and removes any mismatches, generates 1:1 mapping. \n",
    "- This object can be queried using *data.golden* or *data.dataset* or *data.dataset.texts* based on requirement\n",
    "\n",
    "### Files:\n",
    "- Download documents (ibocr or ibdoc files) after running flow on raw files here https://dogfood.instabase.com/dlluncor/lending-club/fs/Instabase%20Drive/workspace-us-markets/w2/data-500/input/. Use s2_map_records\n",
    "- Sample flow outputs and processed goldens here: https://drive.google.com/drive/folders/1h1eHP1Jy8FmRoCehfKQ9dblIwJ8OEmwC?usp=sharing. Download and specify local directory path in code below and set *is_local* = True\n",
    "- Alternatively, specify instabase drive paths and set *is_local* as False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Loading dataset from /Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records\nINFO:root:142 files loaded\nINFO:root:Converting IBOCR/IBDOC to raw texts\nINFO:root:Loading goldens from /Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv\nINFO:root:Total files Goldens: (154, 25)\nINFO:root:Total files found in the source with unique index: (142, 25)\n"
    }
   ],
   "source": [
    "W2_DATA = [\n",
    "   '/Users/ahsaasbajaj/Documents/Data/w2-instabase/flow/s2_map_records'\n",
    "]\n",
    "W2_GOLDEN = [\n",
    "   '/Users/ahsaasbajaj/Documents/Data/w2-instabase/golden/goldens.csv'\n",
    "]\n",
    "\n",
    "GOLDEN_CONFIG = {\n",
    "    'path': W2_GOLDEN,\n",
    "    'is_local': True,\n",
    "    'index_field_name':'filename',\n",
    "    'file_type': 'csv',\n",
    "    'identifier': 'file'\n",
    "}\n",
    "DATASET_CONFIG = {\n",
    "    'path': W2_DATA,\n",
    "    'is_local': True, \n",
    "    'file_type': 'ibocr',\n",
    "    'identifier': lambda path: os.path.basename(path).split('.ibocr')[0],\n",
    "    'convert2txt': True\n",
    "}\n",
    "\n",
    "data = DataCuration(ACCESS_TOKEN, DATASET_CONFIG, GOLDEN_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Goldens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                               employee_ssn  box5_medicare_wages  \\\nfilename                                                           \nlast_year_w2_1493334985571.PDF  561-87-0728             36505.83   \nlast_year_w2_1493334989467.PDF  408-31-3195             51350.25   \nlast_year_w2_1493334998968.PDF  261-77-1595            105916.49   \nlast_year_w2_1493335006405.PDF  452-93-6475             35987.53   \nlast_year_w2_1493752474038.PDF  365-04-7683             85245.86   \n\n                                box3_ss_wage  box6_medicare_withholding  \\\nfilename                                                                  \nlast_year_w2_1493334985571.PDF      36505.83                     529.33   \nlast_year_w2_1493334989467.PDF      51350.25                     744.58   \nlast_year_w2_1493334998968.PDF     105916.49                    1535.82   \nlast_year_w2_1493335006405.PDF      35987.53                     521.82   \nlast_year_w2_1493752474038.PDF      85245.86                    1236.06   \n\n                                box4_ss_withholding  box2_fed_withhold  \\\nfilename                                                                 \nlast_year_w2_1493334985571.PDF              2263.36            4093.92   \nlast_year_w2_1493334989467.PDF              3183.72            6940.69   \nlast_year_w2_1493334998968.PDF              6566.82           24471.02   \nlast_year_w2_1493335006405.PDF              2231.23            2814.31   \nlast_year_w2_1493752474038.PDF              5285.24           13629.89   \n\n                               box17_state_income_tax  box1_wage  \\\nfilename                                                           \nlast_year_w2_1493334985571.PDF                 519.22   36505.83   \nlast_year_w2_1493334989467.PDF                    NaN   47242.23   \nlast_year_w2_1493334998968.PDF                    NaN  105916.49   \nlast_year_w2_1493335006405.PDF                    NaN   35987.53   \nlast_year_w2_1493752474038.PDF                3129.87   77722.96   \n\n                                box8_allocated_tips          box14_other  ...  \\\nfilename                                                                  ...   \nlast_year_w2_1493334985571.PDF                  NaN           [\"328.55\"]  ...   \nlast_year_w2_1493334989467.PDF                  NaN                  NaN  ...   \nlast_year_w2_1493334998968.PDF                  NaN                  NaN  ...   \nlast_year_w2_1493335006405.PDF                  NaN                  NaN  ...   \nlast_year_w2_1493752474038.PDF                  NaN  [\"2069.50\", \"9.00\"]  ...   \n\n                               box12c_amount  box12d_code box12d_amount  \\\nfilename                                                                  \nlast_year_w2_1493334985571.PDF           NaN          NaN           NaN   \nlast_year_w2_1493334989467.PDF           NaN          NaN           NaN   \nlast_year_w2_1493334998968.PDF           NaN          NaN           NaN   \nlast_year_w2_1493335006405.PDF           NaN          NaN           NaN   \nlast_year_w2_1493752474038.PDF      10815.96          NaN           NaN   \n\n                                employer_federal_ein document_type  \\\nfilename                                                             \nlast_year_w2_1493334985571.PDF           01-0726495             W2   \nlast_year_w2_1493334989467.PDF           06-1102358             W2   \nlast_year_w2_1493334998968.PDF            36-4248787            W2   \nlast_year_w2_1493335006405.PDF           74-2482708             W2   \nlast_year_w2_1493752474038.PDF           75-2778918             W2   \n\n                                template_name  \\\nfilename                                        \nlast_year_w2_1493334985571.PDF     general_w2   \nlast_year_w2_1493334989467.PDF     general_w2   \nlast_year_w2_1493334998968.PDF     general_w2   \nlast_year_w2_1493335006405.PDF     general_w2   \nlast_year_w2_1493752474038.PDF     general_w2   \n\n                                                                  employer_name  \\\nfilename                                                                          \nlast_year_w2_1493334985571.PDF                                 BROKER SOLUTIONS   \nlast_year_w2_1493334989467.PDF                            FORMAN INDUSTRIES INC   \nlast_year_w2_1493334998968.PDF  YASH-LUJAN CONSULTING INC Y & L CONSULTING, INC   \nlast_year_w2_1493335006405.PDF                  TECO-WESTINGHOUSE MOTOR COMPANY   \nlast_year_w2_1493752474038.PDF                                 FLOWSERVE US INC   \n\n                                   employee_name w2_year  gross_pay  \nfilename                                                             \nlast_year_w2_1493334985571.PDF  PATRICIA HEREDIA  2016.0   39105.41  \nlast_year_w2_1493334989467.PDF   THOMAS V. MOORE  2016.0   51350.25  \nlast_year_w2_1493334998968.PDF   STACY L STUMETZ  2016.0  110240.00  \nlast_year_w2_1493335006405.PDF      HENRY COTTLE  2016.0   43827.05  \nlast_year_w2_1493752474038.PDF  JASON ALLEN JERZ  2016.0   88420.20  \n\n[5 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>employee_ssn</th>\n      <th>box5_medicare_wages</th>\n      <th>box3_ss_wage</th>\n      <th>box6_medicare_withholding</th>\n      <th>box4_ss_withholding</th>\n      <th>box2_fed_withhold</th>\n      <th>box17_state_income_tax</th>\n      <th>box1_wage</th>\n      <th>box8_allocated_tips</th>\n      <th>box14_other</th>\n      <th>...</th>\n      <th>box12c_amount</th>\n      <th>box12d_code</th>\n      <th>box12d_amount</th>\n      <th>employer_federal_ein</th>\n      <th>document_type</th>\n      <th>template_name</th>\n      <th>employer_name</th>\n      <th>employee_name</th>\n      <th>w2_year</th>\n      <th>gross_pay</th>\n    </tr>\n    <tr>\n      <th>filename</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>last_year_w2_1493334985571.PDF</th>\n      <td>561-87-0728</td>\n      <td>36505.83</td>\n      <td>36505.83</td>\n      <td>529.33</td>\n      <td>2263.36</td>\n      <td>4093.92</td>\n      <td>519.22</td>\n      <td>36505.83</td>\n      <td>NaN</td>\n      <td>[\"328.55\"]</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>01-0726495</td>\n      <td>W2</td>\n      <td>general_w2</td>\n      <td>BROKER SOLUTIONS</td>\n      <td>PATRICIA HEREDIA</td>\n      <td>2016.0</td>\n      <td>39105.41</td>\n    </tr>\n    <tr>\n      <th>last_year_w2_1493334989467.PDF</th>\n      <td>408-31-3195</td>\n      <td>51350.25</td>\n      <td>51350.25</td>\n      <td>744.58</td>\n      <td>3183.72</td>\n      <td>6940.69</td>\n      <td>NaN</td>\n      <td>47242.23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>06-1102358</td>\n      <td>W2</td>\n      <td>general_w2</td>\n      <td>FORMAN INDUSTRIES INC</td>\n      <td>THOMAS V. MOORE</td>\n      <td>2016.0</td>\n      <td>51350.25</td>\n    </tr>\n    <tr>\n      <th>last_year_w2_1493334998968.PDF</th>\n      <td>261-77-1595</td>\n      <td>105916.49</td>\n      <td>105916.49</td>\n      <td>1535.82</td>\n      <td>6566.82</td>\n      <td>24471.02</td>\n      <td>NaN</td>\n      <td>105916.49</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>36-4248787</td>\n      <td>W2</td>\n      <td>general_w2</td>\n      <td>YASH-LUJAN CONSULTING INC Y &amp; L CONSULTING, INC</td>\n      <td>STACY L STUMETZ</td>\n      <td>2016.0</td>\n      <td>110240.00</td>\n    </tr>\n    <tr>\n      <th>last_year_w2_1493335006405.PDF</th>\n      <td>452-93-6475</td>\n      <td>35987.53</td>\n      <td>35987.53</td>\n      <td>521.82</td>\n      <td>2231.23</td>\n      <td>2814.31</td>\n      <td>NaN</td>\n      <td>35987.53</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>74-2482708</td>\n      <td>W2</td>\n      <td>general_w2</td>\n      <td>TECO-WESTINGHOUSE MOTOR COMPANY</td>\n      <td>HENRY COTTLE</td>\n      <td>2016.0</td>\n      <td>43827.05</td>\n    </tr>\n    <tr>\n      <th>last_year_w2_1493752474038.PDF</th>\n      <td>365-04-7683</td>\n      <td>85245.86</td>\n      <td>85245.86</td>\n      <td>1236.06</td>\n      <td>5285.24</td>\n      <td>13629.89</td>\n      <td>3129.87</td>\n      <td>77722.96</td>\n      <td>NaN</td>\n      <td>[\"2069.50\", \"9.00\"]</td>\n      <td>...</td>\n      <td>10815.96</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>75-2778918</td>\n      <td>W2</td>\n      <td>general_w2</td>\n      <td>FLOWSERVE US INC</td>\n      <td>JASON ALLEN JERZ</td>\n      <td>2016.0</td>\n      <td>88420.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "data.golden.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split inputs document to generate *candidate phrases*\n",
    "- These are subsequent tokens clustered using whitespacing information\n",
    "- These are used as strings (to train/test) sequence classifiers\n",
    "- Golden person, org names have to be one of these phrases in order to be extracted by this solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Generating candidates for 142 files\n"
    }
   ],
   "source": [
    "PROCESSING_CONFIG = {\n",
    "    'X_DIST_THRESHOLD': 200\n",
    "}\n",
    "\n",
    "data.generate_candidates_phrases(PROCESSING_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Generate Labeled Data)\n",
    "- Specify DATA_ARGS which includes the task and data objects created beforehand\n",
    "- Mention fields of interest (for extraction, classification) in DATA_ARGS\n",
    "- Generate test data from goldens (from actual persons and company names) \n",
    "- Alternately, generate test data from *candidate phrases* produced by *data.generate_candidates_phrases()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:For X_DIST_THRESHOLD configuraion: 200\nINFO:root:total files: 142\nperson names found in candidates: 130\norg names found in candidates: 69\n\n"
    }
   ],
   "source": [
    "DATA_ARGS = {\n",
    "    'task': task,\n",
    "    'dataset': data,\n",
    "    'candidates_fields': {\n",
    "        'person':'employee_name',\n",
    "        'org':'employer_name'\n",
    "    }\n",
    "}\n",
    "\n",
    "data.compare_candidates_and_goldens(DATA_ARGS['candidates_fields'])\n",
    "fe = FeatureEngineeringNER(DATA_ARGS)\n",
    "test_data_from_goldens = fe.generate_test_samples_from_goldens() # single dataframe\n",
    "test_data_from_candidates = fe.generate_test_samples_from_candidates() # dict{'filename' : dataframe}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling (BERT Inference for sequence classification)\n",
    "- Specify model and load fine-tuned model for inference\n",
    "- The model used in this solution was trained using train_NER.ipynb script (or equivalent)\n",
    "- This model uses pretrained BERT Classifier which was later finetuned on publicly available datasets (Kaggle W2 or public lists of names)\n",
    "\n",
    "### Specify TRAINING_ARGS\n",
    "- Mention the class of model, to be used appropriately by back-end huggingface libraries\n",
    "- Mention the number of labels (in case of multi-label classification)\n",
    "- Also supports the use of GPU for deep learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATHS = {\n",
    "    'w2' : '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/w2/no-address/5/model.pt', # trained on public w2 from Kaggle\n",
    "    'public': '/Users/ahsaasbajaj/Documents/Code/ner-hf/sequence-classification/public/no-address/200/model.pt' # trained on public names repo\n",
    "}\n",
    "\n",
    "TRAINING_ARGS = {\n",
    "    'model_file_or_path' : MODEL_PATHS['w2'],\n",
    "    'model_type': 'bert-large-cased',\n",
    "    'num_labels': TASK_CONFIG['num_labels'],\n",
    "    'gpu': False,\n",
    "}\n",
    "\n",
    "model = BERTNER(DATA_ARGS, TRAINING_ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "- Setup model evaluator and evaluate either using test_data generated in Feature Engineering \n",
    "- Runs BERT inference (in classification setting) and extracts predicted person and company names\n",
    "\n",
    "## Evaluation\n",
    "- Use *model.analyze_result()* to compares predictions with goldens.\n",
    "- Also calculates metrics like Recall, Precision, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:inferring BERT classifier for file last_year_w2_1493919644111.PDF\nINFO:root:inferring BERT classifier for file last_year_w2_1494271162294.PDF\nINFO:root:inferring BERT classifier for file last_year_w2_1495120461121.PNG\nINFO:root:inferring BERT classifier for file last_year_w2_1494972980996.PDF\nINFO:root:inferring BERT classifier for file last_year_w2_1494609579761.PDF\nINFO:root:For field person, recall: 1.0000, precision: 0.5067, F1: 0.6726 \nINFO:root:For field org, recall: 0.0000, precision: 0.4000, F1: 0.0000 \nNumber of files:  5\n"
    }
   ],
   "source": [
    "# Predictions\n",
    "# output_golden = model.predict(test_data_from_goldens) # single dataframe \n",
    "\n",
    "# print('Sample outputs: ', output_golden.head())\n",
    "# model.analyze_golden_result(output_golden)\n",
    "\n",
    "\n",
    "# Do only for debugging and getting quick results\n",
    "test_data = FeatureEngineering.get_subset_for_debugging(test_data_from_candidates, sample_size=5)\n",
    "\n",
    "output = model.predict(test_data) # output is a dictionary\n",
    "print('Number of files: ', len(output.keys()))\n",
    "results = model.analyze_result(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "- Specify local path of PDFs to run a quick DEMO\n",
    "- Use DEMO_FILE from the ones samples in above block (and paste in the block below)\n",
    "- This print the extracted person and company names as per the BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:Field type: person\nINFO:root:filename: last_year_w2_1493919644111.PDF\nINFO:root:{'JPMORGAN CHASE BANK', 'JUDITH VILLARREAL'}\nINFO:root:Field type: org\nINFO:root:filename: last_year_w2_1493919644111.PDF\nINFO:root:{'Dept. of the Treasury - IRS', \"oyee's name, a s, and ZIP code\"}\n"
    }
   ],
   "source": [
    "DIR_PATH = '/Users/ahsaasbajaj/Documents/Data/w2-instabase/pdf'\n",
    "\n",
    "# Choose one file from the list printed above (Samples)\n",
    "DEMO_FILE = 'last_year_w2_1493919644111.PDF'\n",
    "\n",
    "FILE_PATH = DIR_PATH + '/' + DEMO_FILE \n",
    "webbrowser.open_new(r'file:' + FILE_PATH)\n",
    "\n",
    "model.demo(results, DEMO_FILE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598016123287",
   "display_name": "Python 3.7.3 64-bit ('3.7.3': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}